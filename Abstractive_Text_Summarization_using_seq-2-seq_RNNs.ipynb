{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wPgirM2YfebU",
    "outputId": "95419fcb-0b7e-4b78-a5da-b2f13784eccb"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 1.x\n",
    "import numpy as np   #Package for scientific computing and dealing with arrays\n",
    "import pandas as pd  #Package providing fast, flexible and expressive data structures\n",
    "import re            #re stands for RegularExpression providing full support for Perl-like Regular Expressions in Python\n",
    "from bs4 import BeautifulSoup   #Package for pulling data out of HTML and XML files\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  #For tokenizing the input sequences\n",
    "from keras.preprocessing.sequence import pad_sequences  #For Padding the seqences to same length\n",
    "from nltk.corpus import stopwords   #For removing filler words\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n",
    "from tensorflow.keras.models import Model  #Helps in grouping the layers into an object with training and inference features\n",
    "from tensorflow.keras.callbacks import EarlyStopping  #Allows training the model on large no. of training epochs & stop once the performance stops improving on validation dataset\n",
    "import warnings  #shows warning message that may arise \n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200) #Setting the data sructure display length\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingDataACL(path: str):\n",
    "    \"\"\"\n",
    "    Read the ACL data\n",
    "    :param path: File path to read the data from\n",
    "    :return: titles, abstracts NOTE: corresponds to y, x for training\n",
    "    \"\"\"\n",
    "    abstractTitles = open(path, \"r\").read()\n",
    "    papers = [paper.split(\"\\n\") for paper in abstractTitles.split(\"\\n\\n\")]\n",
    "    return zip(*papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "colab_type": "code",
    "id": "IrQMf8BUfedA",
    "outputId": "75531afa-76ea-4d28-a715-be461fef5cb3"
   },
   "outputs": [],
   "source": [
    "titles, abstracts = readingDataACL(\"acl_titles_and_abstracts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pFVcCCAWqu59",
    "outputId": "e1de6525-ffd0-4f2f-968f-bbec96c16ac6"
   },
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"Summary\", \"Text\"] #titles, abstracts\n",
    "reviewsData = pd.DataFrame({\"Summary\": titles, \"Text\": abstracts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "cgW9qaJcCW_b",
    "outputId": "b5703189-2cf9-4f5d-bafc-7266222eed14"
   },
   "outputs": [],
   "source": [
    "reviewsData.info() #Getting more info on datatypes and shape of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3bc3e1Dfeld"
   },
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#This the dictionary used for expanding contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rDsJkVHbfes2",
    "outputId": "b8c89634-550e-47b3-913d-f022e03ec3c4"
   },
   "outputs": [],
   "source": [
    "#Text Cleaning\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()  #converts all uppercase characters in the string into lowercase characters and returns it\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text #parses the string into an lxml.html \n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString) #used to replace a string that matches a regular expression instead of perfect match\n",
    "    newString = re.sub('\"','', newString)           \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) #for expanding contractions using the contraction_mapping dictionary    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    if(num==0): \n",
    "      tokens = [w for w in newString.split() if not w in stop_words]  #converting the strings into tokens\n",
    "    else :\n",
    "      tokens = newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                  #removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "#Calling the function\n",
    "cleaned_text = []\n",
    "for t in reviewsData['Text']:\n",
    "    cleaned_text.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping text cleaning for now - probably not much impact for abstracts\n",
    "cleaned_text = reviewsData['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "_iPKSs0Tfe2x",
    "outputId": "0ef479b4-41d7-4c69-ec80-74aeeddff65d"
   },
   "outputs": [],
   "source": [
    "reviewsData['Text'][:10] #Looking at the 'Text' column of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "XiHDDv9_Izul",
    "outputId": "79e7ac4d-64cc-463b-ed16-8f1c72e0fcb5"
   },
   "outputs": [],
   "source": [
    "cleaned_text[:10] #Looking at the Text after removing stop words, special characters , punctuations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vw1UFS1bfe8F"
   },
   "outputs": [],
   "source": [
    "#Summary Cleaning \n",
    "cleaned_summary = []    #Using the text_cleaner function for cleaning summary too\n",
    "for t in reviewsData['Summary']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "yBOuryAKffAD",
    "outputId": "02da7962-9ff0-472c-a2ee-246969a45448"
   },
   "outputs": [],
   "source": [
    "reviewsData['Summary'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "z0SduOc8ffDO",
    "outputId": "01efc60d-7a05-4858-b1ef-44df1251396c"
   },
   "outputs": [],
   "source": [
    "cleaned_summary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0Lo6geFfe6r"
   },
   "outputs": [],
   "source": [
    "reviewsData['Cleaned_Text'] = cleaned_text  #Adding cleaned text to the dataset\n",
    "reviewsData['Cleaned_Summary'] = cleaned_summary  #Adding cleaned summary to the dataset\n",
    "#Dropping Empty Rows\n",
    "reviewsData['Cleaned_Summary'].replace('', np.nan, inplace=True)\n",
    "#Dropping rows with Missing values\n",
    "reviewsData.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "rUZO3csTvWxc",
    "outputId": "c0634676-7143-4924-8605-3aea7df6f8bf"
   },
   "outputs": [],
   "source": [
    "#Before Cleaning\n",
    "print(\"Before Preprocessing:\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Review:\",reviewsData['Text'][i])\n",
    "    print(\"Summary:\",reviewsData['Summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "qiZxHbedfe1p",
    "outputId": "1e8b7dd3-a5fd-425c-bd2b-811c14628b31"
   },
   "outputs": [],
   "source": [
    "#Printing the Cleaned text and summary which will work as input to the model \n",
    "print(\"After Preprocessing:\\n\")\n",
    "for i in range(5):\n",
    "    print(\"Review:\",reviewsData['Cleaned_Text'][i])\n",
    "    print(\"Summary:\",reviewsData['Cleaned_Summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "wzJRTxrSfewf",
    "outputId": "df017e84-d65f-4449-8b6c-881884fb00e0"
   },
   "outputs": [],
   "source": [
    "#Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "#Populating the lists with sentence lengths\n",
    "for i in reviewsData['Cleaned_Text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in reviewsData['Cleaned_Summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kJQdijuYfegm",
    "outputId": "5a5f915e-62ef-452e-d57b-d300f1aaaa65"
   },
   "outputs": [],
   "source": [
    "#Function for getting the Maximum Review length  \n",
    "count=0 \n",
    "for i in reviewsData['Cleaned_Text']:\n",
    "    if(len(i.split())<=35):\n",
    "        count=count+1\n",
    "print(count/len(reviewsData['Cleaned_Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WILwzlQhgdSQ",
    "outputId": "0f2fc601-6ddf-45f7-9586-fd7a78405fb7"
   },
   "outputs": [],
   "source": [
    "#Function for getting the Maximum Summary length\n",
    "count=0\n",
    "for i in reviewsData['Cleaned_Summary']:\n",
    "    if(len(i.split())<=8):\n",
    "        count=count+1\n",
    "print(count/len(reviewsData['Cleaned_Summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cJavH3YgdWI"
   },
   "outputs": [],
   "source": [
    "#From the above data we got an idea about maximum lengths of review and summary\n",
    "max_text_len = 180 # 35\n",
    "max_summary_len = 20 # 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBu0OPxGgdYV"
   },
   "outputs": [],
   "source": [
    "#Adding START and END tags to summary for better decoding\n",
    "cleaned_text =np.array(reviewsData['Cleaned_Text'])\n",
    "cleaned_summary=np.array(reviewsData['Cleaned_Summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmKhWPrsgdcF"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-iHbqd8ogdeO"
   },
   "outputs": [],
   "source": [
    "#Splitting the Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.2,random_state=0,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHUKg3fIgdjH"
   },
   "outputs": [],
   "source": [
    "#Preparing Tokenizer\n",
    "\n",
    "#Text Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#preparing a tokenizer for reviews on training data\n",
    "X_tokenizer = Tokenizer() \n",
    "X_tokenizer.fit_on_texts(list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "K9YrJJHigdo4",
    "outputId": "fe81edaa-e2b2-43df-89ed-2db282c2407a"
   },
   "outputs": [],
   "source": [
    "#Rarewords and their coverage in review\n",
    "thresh = 4  #If a word whose count is less than threshold i.e 4, then it's considered as rare word \n",
    "\n",
    "cnt = 0      #denotes no. of rare words whose count falls below threshold\n",
    "tot_cnt = 0  #denotes size of unique words in the text\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key,value in X_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMviW6OJgdsO"
   },
   "outputs": [],
   "source": [
    "#Defining the Tokenizer with top most common words for reviews\n",
    "\n",
    "#Preparing a Tokenizer for reviews on training data\n",
    "X_tokenizer = Tokenizer(num_words=tot_cnt-cnt)   #provides top most common words\n",
    "X_tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "#Converting text sequences into integer sequences\n",
    "X_train_seq    =   X_tokenizer.texts_to_sequences(X_train) \n",
    "X_test_seq   =   X_tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#Padding zero upto maximum length\n",
    "X_train    =   pad_sequences(X_train_seq,  maxlen = max_text_len, padding = 'post')\n",
    "X_test   =   pad_sequences(X_test_seq, maxlen = max_text_len, padding = 'post')\n",
    "\n",
    "#Size of vocabulary (+1 for padding token)\n",
    "X_voc   =  X_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xxwsrZLugdnP",
    "outputId": "a6981ef6-520d-471c-a2cc-a1493a1fe985"
   },
   "outputs": [],
   "source": [
    "X_voc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-2EhbNrgdhz"
   },
   "outputs": [],
   "source": [
    "#Summary Tokenizer\n",
    "\n",
    "#Preparing a Tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Jz3AyGNzhDEk",
    "outputId": "03ed98b0-b27e-4edf-a830-730851a94d51"
   },
   "outputs": [],
   "source": [
    "#Rarewords and their coverage in summary\n",
    "\n",
    "thresh = 6  ##If a word whose count is less than threshold i.e 6, then it's considered as rare word \n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "freq = 0\n",
    "tot_freq = 0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt+1\n",
    "    tot_freq = tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt = cnt+1\n",
    "        freq = freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcHAzcb5hDGk"
   },
   "outputs": [],
   "source": [
    "#Defining Tokenizer with the most common words in summary\n",
    "\n",
    "#Preparing a tokenizer for summaries on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)  #provides top most common words\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n",
    "\n",
    "#Converting text sequences into integer sequences\n",
    "y_train_seq    =   y_tokenizer.texts_to_sequences(y_train) \n",
    "y_test_seq   =   y_tokenizer.texts_to_sequences(y_test) \n",
    "\n",
    "#Padding zero upto maximum length\n",
    "y_train    =   pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
    "y_test   =   pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LGwrtZf7zwuQ",
    "outputId": "1a0fe785-4e09-459a-8373-892f254e6268"
   },
   "outputs": [],
   "source": [
    "y_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GoihGf-2hDKE",
    "outputId": "04c87cb7-18a2-48cd-dc6b-dd83c8830362"
   },
   "outputs": [],
   "source": [
    "#Checking the length of training data\n",
    "y_tokenizer.word_counts['sostok'],len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seNf04wKhDMv"
   },
   "outputs": [],
   "source": [
    "#Deleting rows containing START and END tokens\n",
    "#For Training set\n",
    "ind=[]\n",
    "for i in range(len(y_train)):\n",
    "    cnt=0\n",
    "    for j in y_train[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_train=np.delete(y_train,ind, axis=0)\n",
    "X_train=np.delete(X_train,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbQ-bOakhDSQ"
   },
   "outputs": [],
   "source": [
    "#For Validation set\n",
    "ind=[]\n",
    "for i in range(len(y_test)):\n",
    "    cnt=0\n",
    "    for j in y_test[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_test=np.delete(y_test,ind, axis=0)\n",
    "X_test=np.delete(X_test,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "class InitialStateLayer(layers.Layer):\n",
    "    def __init__(self, hidden_size, **kwargs):\n",
    "        super(InitialStateLayer, self).__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Use TensorFlow's tf.zeros_like for symbolic tensors\n",
    "        fake_state = tf.zeros_like(inputs)  # Create a tensor of zeros with the same shape as 'inputs'\n",
    "        fake_state = tf.reduce_sum(fake_state, axis=[1, 2])  # Sum over sequence length and feature dimensions\n",
    "        fake_state = tf.expand_dims(fake_state, axis=-1)  # Expand dims to make it (batch_size, 1)\n",
    "        fake_state = tf.tile(fake_state, [1, self.hidden_size])  # Repeat the fake state across the hidden size dimension\n",
    "        return fake_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "# Define the custom energy step function as an RNNCell\n",
    "class EnergyStep(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(EnergyStep, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add weights or any required setup here\n",
    "        super(EnergyStep, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        decoder_out_seq, fake_state_e = inputs\n",
    "        # Define your custom energy computation logic\n",
    "        # Example: simple linear transformation as energy computation\n",
    "        energy = tf.reduce_sum(decoder_out_seq * fake_state_e, axis=-1)\n",
    "        return energy, [energy]  # Return energy and updated state (energy as the new state)\n",
    "\n",
    "# Define the custom context step function as an RNNCell\n",
    "class ContextStep(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ContextStep, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add weights or any required setup here\n",
    "        super(ContextStep, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        e_outputs, fake_state_c = inputs\n",
    "        # Define your custom context computation logic\n",
    "        # Example: simple linear transformation for context computation\n",
    "        context = tf.reduce_sum(e_outputs * fake_state_c, axis=-1)\n",
    "        return context, [context]  # Return context and updated state (context as the new state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7c6DDCDWhDWa"
   },
   "outputs": [],
   "source": [
    "# NEW: Specify tensorflow version #\n",
    "tf2 = False\n",
    "\n",
    "\n",
    "\n",
    "#Model Building\n",
    "\n",
    "#Adding Custom Attention layer \n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer, RNN\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "\n",
    "        if tf2:\n",
    "        # NEW implementation #\n",
    "            # For `fake_state_c`, use axis=-1 (i.e., reduce over the feature dimension)\n",
    "            initial_state_layer_c = InitialStateLayer(hidden_size=encoder_out_seq.shape[-1])\n",
    "            fake_state_c = initial_state_layer_c(encoder_out_seq)\n",
    "            \n",
    "            # For `fake_state_e`, use axis=1 (i.e., reduce over the sequence length)\n",
    "            initial_state_layer_e = InitialStateLayer(hidden_size=encoder_out_seq.shape[1])\n",
    "            fake_state_e = initial_state_layer_e(encoder_out_seq)\n",
    "    \n",
    "    \n",
    "            energy_cell = EnergyStep()\n",
    "            context_cell = ContextStep()\n",
    "    \n",
    "    \n",
    "            # Use RNN with the custom energy cell to compute e_outputs\n",
    "            rnn_energy = RNN(energy_cell, return_state=True, return_sequences=True)\n",
    "            last_out, e_outputs, _ = rnn_energy([decoder_out_seq, fake_state_e])\n",
    "            \n",
    "            # Use RNN with the custom context cell to compute c_outputs\n",
    "            rnn_context = RNN(context_cell, return_state=True, return_sequences=True)\n",
    "            last_out, c_outputs, _ = rnn_context([e_outputs, fake_state_c])\n",
    "\n",
    "\n",
    "        else:\n",
    "        # OLD implementation #\n",
    "            fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "            fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            \n",
    "            \n",
    "            \"\"\" Computing energy outputs \"\"\"\n",
    "            #e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "            last_out, e_outputs, _ = K.rnn(\n",
    "                energy_step, decoder_out_seq, [fake_state_e],\n",
    "            )\n",
    "            \n",
    "            \"\"\" Computing context vectors \"\"\"\n",
    "            last_out, c_outputs, _ = K.rnn(\n",
    "                context_step, e_outputs, [fake_state_c],\n",
    "            )\n",
    "        \n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QBy5PM7MhDQP",
    "outputId": "03f9d700-c055-4d3b-d1b9-7d7b1ae33cda"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()  #Resets all state generated by Keras\n",
    "\n",
    "latent_dim = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(X_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3= LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "#Setting up the Decoder using 'encoder_states' as initial state\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#Embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "#Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "#Concating Attention input and Decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "#Defining the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "colab_type": "code",
    "id": "b5idXyLEgdQl",
    "outputId": "d4873af8-63b7-4339-fe6a-27e2b7e7e0e1"
   },
   "outputs": [],
   "source": [
    "#Visualize the Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqgFcv1sht7R"
   },
   "outputs": [],
   "source": [
    "#Adding Metrics\n",
    "model.compile(optimizer='rmsprop' , loss='sparse_categorical_crossentropy' , metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXsoihrLhuBP"
   },
   "outputs": [],
   "source": [
    "#Adding Callback\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "tPICqxDehuGX",
    "outputId": "97517801-977a-405c-9280-9b728b26e6f6"
   },
   "outputs": [],
   "source": [
    "#Training the Model\n",
    "#%tensorflow_version 1.x\n",
    "history = model.fit([X_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size= 64, validation_data=([X_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "ohK9YLnPhuLH",
    "outputId": "c94206f6-bbf7-4168-ef44-625fdd042782"
   },
   "outputs": [],
   "source": [
    "#Visualizing Accuracy \n",
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['acc'], label='train') \n",
    "pyplot.plot(history.history['val_acc'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "UQcf72bnhuRj",
    "outputId": "d6c60752-0e45-45d3-fb82-680941f470aa"
   },
   "outputs": [],
   "source": [
    "#Visualizing Loss \n",
    "pyplot.plot(history.history['loss'], label='train') \n",
    "pyplot.plot(history.history['val_loss'], label='test') \n",
    "pyplot.legend() \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7wtfEh16huVw"
   },
   "outputs": [],
   "source": [
    "#Building Dictionary for Source Vocabulary\n",
    "reverse_target_word_index=y_tokenizer.index_word \n",
    "reverse_source_word_index=X_tokenizer.index_word \n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "7mDay3BohueE",
    "outputId": "0478b736-55f8-4661-bf5a-ebf3a9b5de72"
   },
   "outputs": [],
   "source": [
    "#Inference/Validation Phase\n",
    "#Encoding the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#Decoder setup\n",
    "#These tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "#Getting the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#Attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "#Final Decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzLFjn-Zhuhd"
   },
   "outputs": [],
   "source": [
    "#Function defining the implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    #Encoding the input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #Populating the first word of target sequence with the start word\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #Exit condition: either hit max length or find stop word\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Updating the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-pLsERkhubk"
   },
   "outputs": [],
   "source": [
    "#Functions to convert an integer sequence to a word sequence for summary as well as reviews \n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UWHUjVHghuZ7",
    "outputId": "e8886fc9-6ebd-4bf7-dc43-679d8b782e0f"
   },
   "outputs": [],
   "source": [
    "#Summaries generated by the model\n",
    "\n",
    "for i in range(0,20):\n",
    "    print(\"Review:\",seq2text(X_train[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_train[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(X_train[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "MNpmmTN5huUi",
    "outputId": "c7127d23-59f3-4778-db66-666727f238f7"
   },
   "outputs": [],
   "source": [
    "#BLEU Score of Training set\n",
    "#n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_train[i])\n",
    "  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n",
    "\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cc9hAjePhuQQ",
    "outputId": "db76d0c7-03b0-4551-fe4c-95305d296403"
   },
   "outputs": [],
   "source": [
    "#4-gram cumulative BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_train[i])\n",
    "  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JGn7Dm_ehuPO",
    "outputId": "4d3e03c4-494a-462b-8c19-b32ae816c533"
   },
   "outputs": [],
   "source": [
    "#cumulative BLEU scores\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_train[i])\n",
    "  candidate = decode_sequence(X_train[i].reshape(1, max_text_len))\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "BdHImnC2ZNWx",
    "outputId": "77bdd5ac-0f62-46f6-ebf8-8113a034ad44"
   },
   "outputs": [],
   "source": [
    "#BLEU Score of Test/Validation set\n",
    "#n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_test[i])\n",
    "  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\n",
    "print(\"Test/Validation Set :\")\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fWTKxT4pcj-p",
    "outputId": "552abfeb-91a5-47a3-f872-2d700d31b569"
   },
   "outputs": [],
   "source": [
    "#4-gram cumulative BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_test[i])\n",
    "  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WzsNOQenckIN",
    "outputId": "c4caab14-1eb4-4f02-db21-5eacc16d2251"
   },
   "outputs": [],
   "source": [
    "#cumulative BLEU scores\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "for i in range(0,1000):\n",
    "  reference = seq2summary(y_test[i])\n",
    "  candidate = decode_sequence(X_test[i].reshape(1, max_text_len))\n",
    "\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "In-House_Abstractive_Text_Summarization(17102135).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Headliner",
   "language": "python",
   "name": "headliner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
