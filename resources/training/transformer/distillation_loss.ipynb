{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T16:46:39.243110Z",
     "start_time": "2025-01-21T16:46:39.240164Z"
    }
   },
   "outputs": [],
   "source": [
    "# author: Michael HÃ¼ppe\n",
    "# date: 11.11.2024\n",
    "# project: resources/transformer.py\n",
    "import tensorflow as tf\n",
    "from resources.training.transformer.encoder import Encoder\n",
    "from resources.training.transformer.decoder import Decoder\n",
    "\n",
    "def Transformer(\n",
    "        context_vocab_size: int = 5000, target_vocab_size: int = 5000,\n",
    "        model_max_length: int = 250,\n",
    "        embedding_dim: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        num_layers_encoder: int = 1, num_layers_decoder: int = 1,\n",
    "        num_heads: int = 1,\n",
    "        positional_embedding: str = \"rope\", use_seperate_embedding: bool = True,\n",
    "        return_attention_scores: bool = False, **kwargs):\n",
    "    \"\"\"\n",
    "    Implementation of a Transformer model after \"Attention is all you need\"\n",
    "    :param context_vocab_size: Vocab size of the context\n",
    "    :param target_vocab_size: Vocab size of the target\n",
    "    :param model_max_length: Maximum length of the\n",
    "    :param embedding_dim: Dimension of the Embedding\n",
    "    :param dropout: Dropout probability after two drop out layers\n",
    "    :param num_layers_encoder: Number of Encoder Layers\n",
    "    :param num_layers_decoder: Number of Encoder Layers\n",
    "    :param num_heads: Number of heads per layer\n",
    "    :param dropout: Dropout probability after two drop out layers\n",
    "    :param positional_embedding: Type of positional embedding to use [absolute, relative, rope, (segment)]\n",
    "    :param use_seperate_embedding: if True, use seperate Embeddings for encoding and decoding\n",
    "    :param return_attention_scores: if True, the attention scores for the encoder and decoder are returned for each layer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model_max_length = max(model_max_length, kwargs[\"context_max_length\"], kwargs[\"target_max_length\"])\n",
    "    encoder_input = tf.keras.Input(shape=(None,), name=\"encoder_input\")\n",
    "    decoder_input = tf.keras.Input(shape=(None,), name=\"decoder_input\")\n",
    "    encoder_embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=context_vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True\n",
    "    )\n",
    "\n",
    "    if num_layers_encoder != 0:\n",
    "        encoder_embedding = encoder_embedding_layer(encoder_input)\n",
    "\n",
    "        x, encoder_attention = Encoder(encoder_embedding, model_max_length, embedding_dim, dropout,\n",
    "                                       num_layers_encoder, num_heads, positional_embedding)(\n",
    "            encoder_embedding)\n",
    "    else:\n",
    "        x = encoder_input\n",
    "        encoder_attention = {}\n",
    "\n",
    "    if use_seperate_embedding or num_layers_encoder == 0:\n",
    "        decoder_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=context_vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            mask_zero=True\n",
    "        )(decoder_input)\n",
    "    else:\n",
    "        decoder_embedding = encoder_embedding_layer(decoder_input)\n",
    "\n",
    "    x, decoder_attention_causal, decoder_attention_causal_cross = Decoder(decoder_embedding, model_max_length,\n",
    "                                                                          embedding_dim, dropout, num_layers_decoder,\n",
    "                                                                          num_heads,\n",
    "                                                                          positional_embedding)([decoder_embedding, x])\n",
    "    x = tf.keras.layers.Dense(target_vocab_size)(x)\n",
    "    # Define outputs based on the return_attention_scores flag\n",
    "    outputs = x\n",
    "    if return_attention_scores:\n",
    "        outputs = (x, [encoder_attention, decoder_attention_causal, decoder_attention_causal_cross])\n",
    "    target = tf.keras.Input(shape=(target_vocab_size,), name=\"target\")\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input, target], outputs=outputs, name=\"Transformer\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "context = tf.random.uniform((1, 350), minval=0, maxval=100, dtype=tf.int32)\n",
    "sample = tf.random.uniform((1, 20), minval=0, maxval=100, dtype=tf.int32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T16:37:58.083559500Z",
     "start_time": "2025-01-21T16:37:58.079153200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KerasTensor shape=(None, 7671), dtype=float32, sparse=False, name=target> <KerasTensor shape=(None, None, 7671), dtype=float32, sparse=False, name=keras_tensor_82> <KerasTensor shape=(None, None), dtype=float32, sparse=False, name=encoder_input> <KerasTensor shape=(None, None), dtype=float32, sparse=False, name=decoder_input>\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 44\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28mprint\u001B[39m(y_true, y_pred, x, context)\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 44\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mCustomLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     47\u001B[0m model\u001B[38;5;241m.\u001B[39mload_weights(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(model_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodelCheckpoint.weights.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "File \u001B[1;32m~\\Master_Informatik\\Semester_3\\MachineLearning\\loadModels\\Lib\\site-packages\\keras\\src\\models\\functional.py:336\u001B[0m, in \u001B[0;36mFunctional.add_loss\u001B[1;34m(self, loss)\u001B[0m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21madd_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, loss):\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;66;03m# Symbolic only. TODO\u001B[39;00m\n\u001B[1;32m--> 336\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def init_model(Model, params):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    :param Model: Model class to init\n",
    "    :param params: Parameters for the model (parameters are handled in the model functions)\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()  # Clearing Keras memory\n",
    "    tf.random.set_seed(params.get(\"SEED\", 69))  # For reproducibility\n",
    "\n",
    "    # TODO: describe the parameters and softcode\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        0.001,\n",
    "        beta_1=0.9, # The exponential decay rate for the 1st moment estimates. Defaults to 0.9\n",
    "        beta_2=0.98, # The exponential decay rate for the 2nd moment estimates. Defaults to 0.999\n",
    "        epsilon=1e-9 # A small constant for numerical stability\n",
    "    )\n",
    "\n",
    "    if isinstance(Model, str):\n",
    "        if Model == \"Transformer\":\n",
    "            Model = Transformer\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    model = Model(**params)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "model_path = r\"C:\\Users\\mhuep\\Master_Informatik\\Semester_3\\MachineLearning\\trained_models\\Transformer\\01_15_2025__17_41_21\"\n",
    "train_params = json.load(open(os.path.join(model_path, \"modelInfo.json\")))\n",
    "model_params = train_params[\"model_parameters\"]\n",
    "target_max_length, context_max_length = model_params[\"target_max_length\"], model_params[\"context_max_length\"]\n",
    "model = init_model(Transformer, model_params)\n",
    "def CustomLoss(y_true, y_pred, x, context):\n",
    "    print(y_true, y_pred, x, context)\n",
    "    return 0\n",
    "\n",
    "model.add_loss( CustomLoss( model.input[2], model.output, model.input[0], model.input[1] ) )\n",
    "model.compile(loss=None, optimizer='adam')\n",
    "\n",
    "model.load_weights(os.path.join(model_path, \"modelCheckpoint.weights.h5\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T16:49:57.174994200Z",
     "start_time": "2025-01-21T16:49:56.509265500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<KerasTensor shape=(None, None, 7671), dtype=float32, sparse=False, name=keras_tensor_82>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-21T16:43:29.249062600Z",
     "start_time": "2025-01-21T16:43:29.243726300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
