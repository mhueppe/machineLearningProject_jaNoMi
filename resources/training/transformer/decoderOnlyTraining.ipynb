{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\mhuep\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 390), dtype=tf.int32, name=None), TensorSpec(shape=(None, 390), dtype=tf.int32, name=None))>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Define custom layers for embeddings and a single transformer block.\n",
    "# =============================================================================\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Token embeddings: converts token ids to vectors.\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        # Positional embeddings: each position in the sequence gets its own vector.\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch, sequence_length)\n",
    "        positions = tf.range(start=0, limit=tf.shape(x)[1], delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # Multi-head self-attention; note the use of a causal mask below.\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, dropout=dropout_rate)\n",
    "        # Feed-forward network.\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training):\n",
    "        # Apply causal self-attention so that each position can only attend to previous ones.\n",
    "        attn_output = self.att(x, x, x, use_causal_mask=True)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Build the decoder-only transformer model.\n",
    "# =============================================================================\n",
    "\n",
    "def create_transformer_model(max_len, vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n",
    "    inputs = keras.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    x = PositionalEmbedding(max_len, vocab_size, embed_dim)(inputs)\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x, training=True)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Sample data: pairs of (abstract, title).\n",
    "# =============================================================================\n",
    "\n",
    "data_samples = [\n",
    "    (\n",
    "        \"In this paper, we propose a novel transformer-based approach to sequence modeling.\",\n",
    "        \"Transformer Approach\"\n",
    "    ),\n",
    "    (\n",
    "        \"We present an analysis of deep neural networks in image recognition tasks.\",\n",
    "        \"Deep Learning in Vision\"\n",
    "    ),\n",
    "    (\n",
    "        \"A comprehensive study on reinforcement learning methods and their applications.\",\n",
    "        \"Reinforcement Learning Study\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# We will combine abstract and title with a special separator token.\n",
    "separator = \" sep \"\n",
    "combined_texts = [abstract + separator + title for abstract, title in data_samples]\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Tokenization using Keras TextVectorization.\n",
    "# =============================================================================\n",
    "\n",
    "max_tokens = 1000\n",
    "max_len = 50  # maximum sequence length for our model\n",
    "vectorizer = layers.TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_len)\n",
    "vectorizer.adapt(combined_texts)\n",
    "\n",
    "# Retrieve the vocabulary; we need to know the id of the separator token.\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "sep_token = separator.strip()  # e.g., \"<sep>\"\n",
    "if sep_token not in vocab:\n",
    "    raise ValueError(\"Separator token not found in vocabulary!\")\n",
    "sep_token_id = vocab.index(sep_token)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Create the training data with loss mask.\n",
    "#\n",
    "# For a GPT-style (decoder-only) training, we input a sequence that is the\n",
    "# concatenation of abstract and title. The target is the same sequence shifted\n",
    "# one token to the left. We then create a sample weight mask so that only tokens\n",
    "# corresponding to the title (i.e. those after the separator) contribute to loss.\n",
    "# =============================================================================\n",
    "\n",
    "def create_inputs_targets(token_seq):\n",
    "    # token_seq shape: (max_len,)\n",
    "    # Find the first occurrence of the separator token.\n",
    "    sep_positions = tf.where(tf.equal(token_seq, sep_token_id))\n",
    "    sep_index = tf.cast(sep_positions[0][0], tf.int32)\n",
    "    inp = token_seq[:-1]    # all tokens except the last one\n",
    "    target = token_seq[1:]  # shifted by one position\n",
    "    seq_length = tf.shape(target)[0]\n",
    "    # For positions j in target, set weight = 1 if (j+1) (the corresponding original token)\n",
    "    # comes after the separator; else weight = 0.\n",
    "    indices = tf.range(seq_length)\n",
    "    weight = tf.cast(indices >= sep_index, tf.float32)\n",
    "    # Also mask out padding tokens (assumed to be 0).\n",
    "    non_padding = tf.cast(tf.not_equal(target, 0), tf.float32)\n",
    "    weight = weight * non_padding\n",
    "    return inp, target, weight\n",
    "\n",
    "# Build a tf.data.Dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(combined_texts)\n",
    "dataset = dataset.map(lambda t: vectorizer(t))\n",
    "dataset = dataset.map(create_inputs_targets)\n",
    "BATCH_SIZE = 2\n",
    "dataset = dataset.shuffle(10).batch(BATCH_SIZE)\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Create and compile the model.\n",
    "# =============================================================================\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = create_transformer_model(max_len, vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.summary()\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Train the model.\n",
    "# =============================================================================\n",
    "\n",
    "model.fit(dataset, epochs=10)\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Inference: Generate a title given an abstract.\n",
    "#\n",
    "# We provide only the abstract plus the separator as a prompt. Then we use a\n",
    "# simple greedy decoding loop to generate a fixed number of tokens. The generated\n",
    "# sequence is decoded and the title part (the words after the separator) is returned.\n",
    "# =============================================================================\n",
    "\n",
    "def generate_title(abstract, max_gen_len=10):\n",
    "    prompt = abstract + separator\n",
    "    # Vectorize the prompt; output shape: (1, sequence_length)\n",
    "    token_seq = vectorizer([prompt])\n",
    "    token_seq = tf.squeeze(token_seq, axis=0)  # shape: (max_len,)\n",
    "    # Remove trailing padding (assuming padding token is 0)\n",
    "    token_seq = token_seq[token_seq != 0]\n",
    "\n",
    "    for _ in range(max_gen_len):\n",
    "        # Pad current sequence to max_len (our model input length)\n",
    "        inp = tf.expand_dims(token_seq, 0)\n",
    "        inp = tf.pad(inp, [[0, 0], [0, max_len - tf.shape(inp)[1]]])\n",
    "        predictions = model(inp, training=False)  # shape: (1, max_len, vocab_size)\n",
    "        # Pick the token at the current last position.\n",
    "        next_token_logits = predictions[0, tf.shape(token_seq)[0]-1, :]\n",
    "        next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "        if next_token == 0:\n",
    "            break  # stop if the model generates a padding token\n",
    "        token_seq = tf.concat([token_seq, [next_token]], axis=0)\n",
    "        if tf.shape(token_seq)[0] >= max_len:\n",
    "            break\n",
    "\n",
    "    # Decode token ids back to words.\n",
    "    inv_vocab = {i: word for i, word in enumerate(vocab)}\n",
    "    generated_words = [inv_vocab.get(int(token), \"\") for token in token_seq.numpy().tolist()]\n",
    "    # Find the separator token and take the words after it as the generated title.\n",
    "    if sep_token in generated_words:\n",
    "        idx = generated_words.index(sep_token)\n",
    "        title_words = generated_words[idx+1:]\n",
    "    else:\n",
    "        title_words = generated_words\n",
    "    return \" \".join(title_words).strip()\n",
    "\n",
    "# Test the inference routine.\n",
    "test_abstract = (\"This study introduces a new method for natural language understanding \"\n",
    "                 \"using attention mechanisms to better capture contextual dependencies.\")\n",
    "generated_title = generate_title(test_abstract)\n",
    "print(\"Generated Title:\", generated_title)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-10T21:04:25.078977400Z",
     "start_time": "2025-03-10T21:04:23.380671600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
